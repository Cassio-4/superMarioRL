{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_mario.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7asy7Eovq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install import-ipynb\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdcBjZuby5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import import_ipynb\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "Drive = GoogleDrive(gauth)\n",
        "#importando wrapper\n",
        "# https://drive.google.com/open?id=1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA\n",
        "my_wrapper = Drive.CreateFile({'id':'1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA'})\n",
        "my_wrapper.GetContentFile('env_wrapper.ipynb')\n",
        "import env_wrapper as wrappers\n",
        "#importando modelo de rede dqn\n",
        "#https://drive.google.com/open?id=14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd\n",
        "dqn_model = Drive.CreateFile({'id':'14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd'})\n",
        "dqn_model.GetContentFile('dqn.ipynb')\n",
        "import dqn as dqn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q86nQjqFPl08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/My Drive/DQN/Logs\"\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dqr0pbfZDnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-RBqvaZGy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
        "# Valor de Gamma usado na aproximação de Bellman\n",
        "GAMMA = 0.99\n",
        "# O tamanho da 'batch' amostrada do Replay Buffer\n",
        "BATCH_SIZE = 32\n",
        "# Capacidade máxima do Replay Buffer\n",
        "REPLAY_SIZE = 10000\n",
        "# Número de steps que esperamos antes de começar a treinar para popular o Replay Buffer\n",
        "REPLAY_START_SIZE = 10000\n",
        "# Taxa de aprendizado usada pelo otimizador Adam, que é usado nesse código\n",
        "LEARNING_RATE = 1e-4\n",
        "# Frequência de sincronização dos pesos do modelo de treino para o modelo alvo.\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "# Estamos usando Double Deep Q-learning?\n",
        "DOUBLE_DQN = True\n",
        "\n",
        "\"\"\" Parâmetros de declínio de Epsilon. No começo do treinamento começamos com\n",
        "Epsilon=1.0, ou seja, com ações totalmente aleatórias. Depois, durante os 4k\n",
        "primeiros jogos, Epsilon decai linearmente para 0.02\"\"\"\n",
        "EPSILON_DECAY_LAST_GAME = 5000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "\"\"\" Pasta onde salvaremos os logs  \"\"\"\n",
        "train_log_dir = \"/content/drive/My Drive/DQN/Logs/\"\n",
        "\"\"\"Parametros de retomar treino\"\"\"\n",
        "LOAD_NET = True\n",
        "LOAD_REPLAY_BUFFER = True\n",
        "step_index = 319378\n",
        "games_played = 1000\n",
        "epsilon = 0.8\n",
        "\"\"\"Bufferizando para depois inserir no log\"\"\"\n",
        "log_array = []\n",
        "action_reward_x_y_eps_loss_perStep_array = []\n",
        "games_array = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptbVqOXou6yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Código referente ao Replay Buffer. A cada Step no ambiente colocamos a \n",
        "transição no Buffer, mantendo apenas um número fixo de Steps, nesse caso 10k \n",
        "transições. Para treinamento, selecionamos aleatóriamente um lote (batch) de\n",
        "transições do Replay Buffer, o que permite que quebremos a correlação entre \n",
        "passos subsequentes no ambiente.\n",
        "\"\"\"\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.bool), np.array(next_states)\n",
        "\n",
        "\"\"\" \n",
        "Agente:interage com o ambiente e salva o resultado da interação no \n",
        "experience replay buffer\n",
        "\"\"\"\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Na inicialização do Agente guardamos referências para o ambiente (env) e \n",
        "    para o experience replay buffer, registrando a observação(observation) atual\n",
        "    e a recompensa total acumulada até então.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "    \"\"\"\n",
        "    Faz um step no ambiente e guarda o resultado no Buffer. Com probabilidade \n",
        "    Epsilon tomamos uma ação aleatória, caso contrário utilizamos o modelo para \n",
        "    obter os Q-values para todas as possíveis ações e escolhemos a melhor.\n",
        "    \"\"\"\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            epsilon_action = True\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            epsilon_action = False\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "        action_reward_x_y_eps_loss_perStep_array.append(action)\n",
        "        \"\"\"\n",
        "        Passamos a ação escolhida para o ambiente e pegamos a próxima observation\n",
        "        e recompensa, guardamos os dados no experience buffer e tratamos o fim-\n",
        "        -de-episodio. O resultado dessa função é a recompensa total acumulada se\n",
        "        chegamos ao fim-de-episodio com esse step ou None caso contrário.\n",
        "        \"\"\"\n",
        "        new_state, reward, is_done, info = self.env.step(action)\n",
        "        #log\n",
        "        action_reward_x_y_eps_loss_perStep_array.append(reward)\n",
        "        action_reward_x_y_eps_loss_perStep_array.append(info.get(\"x_pos\"))\n",
        "        action_reward_x_y_eps_loss_perStep_array.append(info.get(\"y_pos\"))\n",
        "        if epsilon_action:\n",
        "            action_reward_x_y_eps_loss_perStep_array.append(1)\n",
        "        else:\n",
        "            action_reward_x_y_eps_loss_perStep_array.append(0)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward, info\n",
        "\n",
        "\"\"\"\n",
        "Função que calcula a Loss para a batch amostrada.\n",
        "Argumentos: a batch como uma tupla de arrays (método sample() do experience \n",
        "buffer), nossa rede de treino e a rede alvo, periodicamente sincronizada com a \n",
        "de treino.\n",
        "\"\"\"\n",
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    \"\"\"\n",
        "    Empacota arrays individuais NumPy com dados do lote(batch) em tensores\n",
        "    PyTorch e copia para a GPU\n",
        "    \"\"\"\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "    \"\"\"\n",
        "    Passa as observations para a primeira rede e extrai os Q-Values específicos\n",
        "    para as ações tomadas (?) usando a operação de tensor gather().\n",
        "    \"\"\"\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    \"\"\"\n",
        "    Aplica a rede alvo para as observações de próximo estado e calcula o máximo \n",
        "    Q-Value ao longo da mesma dimensão 1 de ação(?). A função max() retorna \n",
        "    ambos os valores máximos e seus índices (max e argmax), entretanto neste \n",
        "    caso, estamos apenas interesados nos valores, então pegamos a primeira \n",
        "    entrada do resultado\n",
        "    \"\"\"\n",
        "    #Se estivermos treinando uma double dqn calculamos os q values assim:\n",
        "    if DOUBLE_DQN:\n",
        "        next_state_actions = net(next_states_v).max(1)[1]\n",
        "        next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
        "    #se for uma dqn normal, calculamos assim\n",
        "    else:\n",
        "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    \"\"\"\n",
        "    Aqui há um ponto simples porém, importante: se a transição no batch é do \n",
        "    último step do episódio, então nosso valor da ação não tem uma recompensa com\n",
        "    desconto do próximo estado, visto que não há próximo estado de onde tirar \n",
        "    uma recompensa. Pode parecer pequeno mas é muito importante na prática, sem \n",
        "    isso o treino NÃO irá convergir\n",
        "    \"\"\"\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "    \"\"\"\n",
        "    Calcula o valor da aproximação de Bellman e o erro quadrático médio (MSE)\n",
        "    (loss)\n",
        "    \"\"\"\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "def write_logs(steps, games):\n",
        "    with open('steps_epsilon_loss.csv', 'a', newline='') as steps_log:\n",
        "        steps_writer = csv.writer(steps_log)\n",
        "        for entry in steps:\n",
        "            #action, reward, x, y, isEpsilon, loss\n",
        "            steps_writer.writerow([entry[0], entry[1], entry[2], entry[3], entry[4], entry[5]])\n",
        "        steps_log.close()\n",
        "        log_array.clear()\n",
        "    \n",
        "    with open('games_log.csv', 'a', newline='') as games_log:\n",
        "        games_writer = csv.writer(games_log)\n",
        "        for entry in games:\n",
        "            #reward, epsilon, time_spent_game, info.get(\"x_pos\"), info.get(\"flag_get\"), ingameScore\n",
        "            games_writer.writerow([entry[0], entry[1], entry[2], entry[3], entry[4], entry[5]])\n",
        "        games_log.close()\n",
        "        games_array.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbZn2CfYYEM",
        "colab_type": "code",
        "outputId": "c034bd63-2864-4fa6-91cb-aea90bebcfbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Se cuda estiver disponível setamos device como cuda\n",
        "    device = torch.device('cuda')\n",
        "    print(device)\n",
        "    if DOUBLE_DQN:\n",
        "        print(\"Training double dqn\")\n",
        "    else:\n",
        "        print(\"Training dqn\")\n",
        "    \"\"\"\n",
        "    Cria ambiente (env) com todos os wrappers aplicados, a rede neural que \n",
        "    será treinada e a rede alvo com a mesma arquitetura. No início serão \n",
        "    inicializadas com pesos diferentes, mas isso não importa visto que serão \n",
        "    sincronizadas a cada SYNC_TARGET_FRAMES\n",
        "    \"\"\"\n",
        "    env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
        "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)#.to(device)\n",
        "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)#.to(device)\n",
        "    #Se vamos retomar o treino, carregamos a rede com os parametros salvos e \n",
        "    #sincronizamos com a target_network \n",
        "    if LOAD_NET:\n",
        "        print(\"Reloading net: game %d, step_index %d, epsilon %.2f\" % \n",
        "            (games_played, step_index, epsilon))\n",
        "        net.load_state_dict(torch.load('best.dat', map_location=lambda storage, loc: storage))\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "    net.to(device)\n",
        "    tgt_net.to(device)\n",
        "    \"\"\"\n",
        "    Cria o experience replay buffer de tamanho REPLAY_SIZE e o passa para o agente.\n",
        "    Epsilon é inicializado com 1.0, mas decaira a cada jogo.\n",
        "    \"\"\"\n",
        "    #Se retomar treino, carrega o replay buffer\n",
        "    if LOAD_REPLAY_BUFFER:\n",
        "        print(\"Reloading Replay Buffer\")\n",
        "        with open('experienceBuffer', 'rb') as filehandler: \n",
        "            buffer = pickle.load(filehandler)\n",
        "    else:\n",
        "        buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    \"\"\"\n",
        "    A última parte antes do loop de treino consiste em: criar o otimizador e \n",
        "    variavéis de tempo.Toda vez que a executarmos um numero arbitrario de jogos,\n",
        "    salvamos o modelo em um arquivo.\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    ts = time.time()\n",
        "    step_start = step_index\n",
        "\n",
        "    while True:\n",
        "        step_index += 1\n",
        "        \"\"\"\n",
        "        Faz o agente dar um único step (usando a rede atual e o valor de epsilon).\n",
        "        Essa função não retorna None apenas se este Step for o último do episódio,\n",
        "        nesse caso, relatamos nosso progresso.\n",
        "        \"\"\"\n",
        "        reward, info = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            games_played += 1\n",
        "            \"\"\"\n",
        "            Epsilon decairá linearmente durante o número de jogos dados \n",
        "            (EPSILON_DECAY_LAST_GAME) e será mantido no mesmo nível que EPSILON_FINAL.\n",
        "            \"\"\"\n",
        "            epsilon = max(EPSILON_FINAL, EPSILON_START - games_played / EPSILON_DECAY_LAST_GAME)\n",
        "            steps_done = step_index - step_start;\n",
        "            step_start = step_index \n",
        "            time_spent_game = time.time() - ts\n",
        "            ts = time.time()\n",
        "            speed = steps_done / time_spent_game\n",
        "            print(\"steps[%d]: game [%d], reward [%.3f], eps [%.2f], time spent %.2f s, speed %d steps/s\" % (\n",
        "                step_index, games_played, reward, epsilon, time_spent_game, speed\n",
        "            ))\n",
        "            games_array.append([reward, epsilon, time_spent_game, info.get(\"x_pos\"), info.get(\"flag_get\"), info.get(\"score\")])\n",
        "            # Salva a rede, replay_buffer e preenche logs a cada 'x' jogos\n",
        "            if games_played % 100 == 0:\n",
        "                torch.save(net.state_dict(), 'best.dat')\n",
        "                with open('experienceBuffer', 'wb') as filehandler:\n",
        "                    pickle.dump(buffer, filehandler)\n",
        "                write_logs(log_array, games_array)\n",
        "                print(\"Checkpoint! Game: %d, reward: %.3f, epsilon %.2f\" % \n",
        "                    (games_played, reward, epsilon))\n",
        "                with open('checkpoint.txt', 'w', newline='') as txt_log:\n",
        "                    txt_log.write(\"Checkpoint! Game #%d, step_index: %d, eps: %.2f\" \n",
        "                                  % (games_played, step_index, epsilon))\n",
        "            if games_played > 20000:\n",
        "                print(\"Done %d games_played!\" % games_played)\n",
        "                break\n",
        "        \"\"\"\n",
        "        Checa se o Replay Buffer é grande o bastante para começar o treino. No\n",
        "        começo devemos esperar REPLAY_SIZE transições. A condição seguinte \n",
        "        sincroniza os parâmetros da rede principal e da rede alvo a cada SYNC_TARGET_FRAMES.\n",
        "        \"\"\"\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            action_reward_x_y_eps_loss_perStep_array.clear()\n",
        "            continue\n",
        "        if step_index % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "        \"\"\"\n",
        "        A última parte do loop de treino é simples, mas requer a maior parte \n",
        "        para executar: zera os gradientes, amostra lotes de dados do experience \n",
        "        replay buffer, calcula perca (loss) e faz o passo de otimização para \n",
        "        minimizar a perca.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # log\n",
        "        action_reward_x_y_eps_loss_perStep_array.append(loss_t.item())\n",
        "        log_array.append(action_reward_x_y_eps_loss_perStep_array.copy())\n",
        "        action_reward_x_y_eps_loss_perStep_array.clear()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Training double dqn\n",
            "Reloading net: game 1000, step_index 319378, epsilon 0.80\n",
            "Reloading Replay Buffer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "steps[319426]: game [1001], reward [211.000], eps [0.80], time spent 1.44 s, speed 33 steps/s\n",
            "steps[319457]: game [1002], reward [248.000], eps [0.80], time spent 0.88 s, speed 35 steps/s\n",
            "steps[320086]: game [1003], reward [1251.000], eps [0.80], time spent 17.85 s, speed 35 steps/s\n",
            "steps[320145]: game [1004], reward [213.000], eps [0.80], time spent 1.65 s, speed 35 steps/s\n",
            "steps[320434]: game [1005], reward [608.000], eps [0.80], time spent 8.08 s, speed 35 steps/s\n",
            "steps[320588]: game [1006], reward [746.000], eps [0.80], time spent 4.34 s, speed 35 steps/s\n",
            "steps[320658]: game [1007], reward [202.000], eps [0.80], time spent 1.95 s, speed 35 steps/s\n",
            "steps[321768]: game [1008], reward [1154.000], eps [0.80], time spent 31.22 s, speed 35 steps/s\n",
            "steps[323376]: game [1009], reward [844.000], eps [0.80], time spent 45.16 s, speed 35 steps/s\n",
            "steps[323806]: game [1010], reward [986.000], eps [0.80], time spent 12.05 s, speed 35 steps/s\n",
            "steps[324656]: game [1011], reward [996.000], eps [0.80], time spent 23.86 s, speed 35 steps/s\n",
            "steps[324696]: game [1012], reward [239.000], eps [0.80], time spent 1.11 s, speed 36 steps/s\n",
            "steps[325553]: game [1013], reward [1207.000], eps [0.80], time spent 24.06 s, speed 35 steps/s\n",
            "steps[325671]: game [1014], reward [599.000], eps [0.80], time spent 3.28 s, speed 35 steps/s\n",
            "steps[326126]: game [1015], reward [719.000], eps [0.80], time spent 12.83 s, speed 35 steps/s\n",
            "steps[326162]: game [1016], reward [248.000], eps [0.80], time spent 1.01 s, speed 35 steps/s\n",
            "steps[326615]: game [1017], reward [691.000], eps [0.80], time spent 12.75 s, speed 35 steps/s\n",
            "steps[326999]: game [1018], reward [721.000], eps [0.80], time spent 10.80 s, speed 35 steps/s\n",
            "steps[327118]: game [1019], reward [593.000], eps [0.80], time spent 3.32 s, speed 35 steps/s\n",
            "steps[328744]: game [1020], reward [1020.000], eps [0.80], time spent 45.50 s, speed 35 steps/s\n",
            "steps[328918]: game [1021], reward [633.000], eps [0.80], time spent 4.85 s, speed 35 steps/s\n",
            "steps[329139]: game [1022], reward [604.000], eps [0.80], time spent 6.17 s, speed 35 steps/s\n",
            "steps[329374]: game [1023], reward [571.000], eps [0.80], time spent 6.55 s, speed 35 steps/s\n",
            "steps[329412]: game [1024], reward [232.000], eps [0.80], time spent 1.05 s, speed 36 steps/s\n",
            "steps[329448]: game [1025], reward [242.000], eps [0.80], time spent 1.00 s, speed 36 steps/s\n",
            "steps[329579]: game [1026], reward [619.000], eps [0.79], time spent 3.66 s, speed 35 steps/s\n",
            "steps[330339]: game [1027], reward [928.000], eps [0.79], time spent 21.39 s, speed 35 steps/s\n",
            "steps[330447]: game [1028], reward [585.000], eps [0.79], time spent 3.01 s, speed 35 steps/s\n",
            "steps[330967]: game [1029], reward [738.000], eps [0.79], time spent 14.56 s, speed 35 steps/s\n",
            "steps[331006]: game [1030], reward [237.000], eps [0.79], time spent 1.09 s, speed 35 steps/s\n",
            "steps[331094]: game [1031], reward [609.000], eps [0.79], time spent 2.47 s, speed 35 steps/s\n",
            "steps[331133]: game [1032], reward [249.000], eps [0.79], time spent 1.08 s, speed 36 steps/s\n",
            "steps[331483]: game [1033], reward [773.000], eps [0.79], time spent 9.81 s, speed 35 steps/s\n",
            "steps[331522]: game [1034], reward [237.000], eps [0.79], time spent 1.08 s, speed 35 steps/s\n",
            "steps[331679]: game [1035], reward [620.000], eps [0.79], time spent 4.40 s, speed 35 steps/s\n",
            "steps[331724]: game [1036], reward [226.000], eps [0.79], time spent 1.26 s, speed 35 steps/s\n",
            "steps[331817]: game [1037], reward [618.000], eps [0.79], time spent 2.59 s, speed 35 steps/s\n",
            "steps[331848]: game [1038], reward [247.000], eps [0.79], time spent 0.86 s, speed 35 steps/s\n",
            "steps[331892]: game [1039], reward [229.000], eps [0.79], time spent 1.21 s, speed 36 steps/s\n",
            "steps[332009]: game [1040], reward [602.000], eps [0.79], time spent 3.26 s, speed 35 steps/s\n",
            "steps[332230]: game [1041], reward [597.000], eps [0.79], time spent 6.15 s, speed 35 steps/s\n",
            "steps[332273]: game [1042], reward [246.000], eps [0.79], time spent 1.20 s, speed 35 steps/s\n",
            "steps[332831]: game [1043], reward [1239.000], eps [0.79], time spent 15.61 s, speed 35 steps/s\n",
            "steps[333015]: game [1044], reward [585.000], eps [0.79], time spent 5.20 s, speed 35 steps/s\n",
            "steps[333118]: game [1045], reward [630.000], eps [0.79], time spent 2.86 s, speed 35 steps/s\n",
            "steps[333588]: game [1046], reward [672.000], eps [0.79], time spent 13.20 s, speed 35 steps/s\n",
            "steps[333631]: game [1047], reward [225.000], eps [0.79], time spent 1.21 s, speed 35 steps/s\n",
            "steps[334430]: game [1048], reward [589.000], eps [0.79], time spent 22.50 s, speed 35 steps/s\n",
            "steps[334468]: game [1049], reward [246.000], eps [0.79], time spent 1.05 s, speed 36 steps/s\n",
            "steps[334522]: game [1050], reward [225.000], eps [0.79], time spent 1.50 s, speed 35 steps/s\n",
            "steps[334557]: game [1051], reward [236.000], eps [0.79], time spent 0.98 s, speed 35 steps/s\n",
            "steps[334587]: game [1052], reward [245.000], eps [0.79], time spent 0.84 s, speed 35 steps/s\n",
            "steps[334732]: game [1053], reward [593.000], eps [0.79], time spent 4.06 s, speed 35 steps/s\n",
            "steps[334782]: game [1054], reward [213.000], eps [0.79], time spent 1.40 s, speed 35 steps/s\n",
            "steps[334824]: game [1055], reward [236.000], eps [0.79], time spent 1.17 s, speed 35 steps/s\n",
            "steps[334866]: game [1056], reward [245.000], eps [0.79], time spent 1.18 s, speed 35 steps/s\n",
            "steps[334896]: game [1057], reward [247.000], eps [0.79], time spent 0.83 s, speed 35 steps/s\n",
            "steps[334959]: game [1058], reward [176.000], eps [0.79], time spent 1.77 s, speed 35 steps/s\n",
            "steps[335102]: game [1059], reward [713.000], eps [0.79], time spent 4.00 s, speed 35 steps/s\n",
            "steps[335513]: game [1060], reward [999.000], eps [0.79], time spent 11.53 s, speed 35 steps/s\n",
            "steps[335608]: game [1061], reward [632.000], eps [0.79], time spent 2.65 s, speed 35 steps/s\n",
            "steps[335667]: game [1062], reward [225.000], eps [0.79], time spent 1.67 s, speed 35 steps/s\n",
            "steps[336297]: game [1063], reward [1234.000], eps [0.79], time spent 17.91 s, speed 35 steps/s\n",
            "steps[336356]: game [1064], reward [181.000], eps [0.79], time spent 1.66 s, speed 35 steps/s\n",
            "steps[336475]: game [1065], reward [584.000], eps [0.79], time spent 3.34 s, speed 35 steps/s\n",
            "steps[336541]: game [1066], reward [205.000], eps [0.79], time spent 1.88 s, speed 35 steps/s\n",
            "steps[336616]: game [1067], reward [160.000], eps [0.79], time spent 2.11 s, speed 35 steps/s\n",
            "steps[336676]: game [1068], reward [208.000], eps [0.79], time spent 1.67 s, speed 35 steps/s\n",
            "steps[337480]: game [1069], reward [918.000], eps [0.79], time spent 22.91 s, speed 35 steps/s\n",
            "steps[337532]: game [1070], reward [224.000], eps [0.79], time spent 1.47 s, speed 35 steps/s\n",
            "steps[337585]: game [1071], reward [208.000], eps [0.79], time spent 1.49 s, speed 35 steps/s\n",
            "steps[338589]: game [1072], reward [880.000], eps [0.79], time spent 28.36 s, speed 35 steps/s\n",
            "steps[338640]: game [1073], reward [243.000], eps [0.79], time spent 1.43 s, speed 35 steps/s\n",
            "steps[338673]: game [1074], reward [250.000], eps [0.79], time spent 0.93 s, speed 35 steps/s\n",
            "steps[338752]: game [1075], reward [638.000], eps [0.79], time spent 2.23 s, speed 35 steps/s\n",
            "steps[338792]: game [1076], reward [237.000], eps [0.78], time spent 1.12 s, speed 35 steps/s\n",
            "steps[338832]: game [1077], reward [239.000], eps [0.78], time spent 1.12 s, speed 35 steps/s\n",
            "steps[339112]: game [1078], reward [737.000], eps [0.78], time spent 7.92 s, speed 35 steps/s\n",
            "steps[339349]: game [1079], reward [619.000], eps [0.78], time spent 6.67 s, speed 35 steps/s\n",
            "steps[339564]: game [1080], reward [720.000], eps [0.78], time spent 6.04 s, speed 35 steps/s\n",
            "steps[339606]: game [1081], reward [229.000], eps [0.78], time spent 1.18 s, speed 35 steps/s\n",
            "steps[340338]: game [1082], reward [1201.000], eps [0.78], time spent 20.52 s, speed 35 steps/s\n",
            "steps[340511]: game [1083], reward [630.000], eps [0.78], time spent 4.85 s, speed 35 steps/s\n",
            "steps[340573]: game [1084], reward [194.000], eps [0.78], time spent 1.73 s, speed 35 steps/s\n",
            "steps[341034]: game [1085], reward [1267.000], eps [0.78], time spent 12.91 s, speed 35 steps/s\n",
            "steps[341076]: game [1086], reward [231.000], eps [0.78], time spent 1.17 s, speed 35 steps/s\n",
            "steps[341593]: game [1087], reward [1245.000], eps [0.78], time spent 14.56 s, speed 35 steps/s\n",
            "steps[341634]: game [1088], reward [231.000], eps [0.78], time spent 1.16 s, speed 35 steps/s\n",
            "steps[341677]: game [1089], reward [227.000], eps [0.78], time spent 1.21 s, speed 35 steps/s\n",
            "steps[341882]: game [1090], reward [582.000], eps [0.78], time spent 5.71 s, speed 35 steps/s\n",
            "steps[343290]: game [1091], reward [1448.000], eps [0.78], time spent 39.11 s, speed 35 steps/s\n",
            "steps[343694]: game [1092], reward [768.000], eps [0.78], time spent 11.27 s, speed 35 steps/s\n",
            "steps[343729]: game [1093], reward [244.000], eps [0.78], time spent 0.99 s, speed 35 steps/s\n",
            "steps[343995]: game [1094], reward [1304.000], eps [0.78], time spent 7.53 s, speed 35 steps/s\n",
            "steps[344044]: game [1095], reward [221.000], eps [0.78], time spent 1.35 s, speed 36 steps/s\n",
            "steps[344607]: game [1096], reward [971.000], eps [0.78], time spent 15.71 s, speed 35 steps/s\n",
            "steps[344898]: game [1097], reward [1023.000], eps [0.78], time spent 8.16 s, speed 35 steps/s\n",
            "steps[344972]: game [1098], reward [166.000], eps [0.78], time spent 2.07 s, speed 35 steps/s\n",
            "steps[345078]: game [1099], reward [624.000], eps [0.78], time spent 2.97 s, speed 35 steps/s\n",
            "steps[345125]: game [1100], reward [235.000], eps [0.78], time spent 1.32 s, speed 35 steps/s\n",
            "Checkpoint! Game: 1100, reward: 235.000, epsilon 0.78\n",
            "steps[345364]: game [1101], reward [764.000], eps [0.78], time spent 15.51 s, speed 15 steps/s\n",
            "steps[345541]: game [1102], reward [632.000], eps [0.78], time spent 5.44 s, speed 32 steps/s\n",
            "steps[345741]: game [1103], reward [626.000], eps [0.78], time spent 6.26 s, speed 31 steps/s\n",
            "steps[345780]: game [1104], reward [229.000], eps [0.78], time spent 1.22 s, speed 32 steps/s\n",
            "steps[345906]: game [1105], reward [602.000], eps [0.78], time spent 3.83 s, speed 32 steps/s\n",
            "steps[346281]: game [1106], reward [998.000], eps [0.78], time spent 10.75 s, speed 34 steps/s\n",
            "steps[346411]: game [1107], reward [573.000], eps [0.78], time spent 3.67 s, speed 35 steps/s\n",
            "steps[347469]: game [1108], reward [1155.000], eps [0.78], time spent 29.60 s, speed 35 steps/s\n",
            "steps[347578]: game [1109], reward [598.000], eps [0.78], time spent 3.13 s, speed 34 steps/s\n",
            "steps[347960]: game [1110], reward [683.000], eps [0.78], time spent 10.75 s, speed 35 steps/s\n",
            "steps[347998]: game [1111], reward [241.000], eps [0.78], time spent 1.06 s, speed 35 steps/s\n",
            "steps[348510]: game [1112], reward [962.000], eps [0.78], time spent 14.40 s, speed 35 steps/s\n",
            "steps[348686]: game [1113], reward [598.000], eps [0.78], time spent 4.94 s, speed 35 steps/s\n",
            "steps[348804]: game [1114], reward [593.000], eps [0.78], time spent 3.30 s, speed 35 steps/s\n",
            "steps[348984]: game [1115], reward [632.000], eps [0.78], time spent 5.04 s, speed 35 steps/s\n",
            "steps[349632]: game [1116], reward [953.000], eps [0.78], time spent 18.04 s, speed 35 steps/s\n",
            "steps[349783]: game [1117], reward [588.000], eps [0.78], time spent 4.21 s, speed 35 steps/s\n",
            "steps[349899]: game [1118], reward [618.000], eps [0.78], time spent 3.21 s, speed 36 steps/s\n",
            "steps[350064]: game [1119], reward [635.000], eps [0.78], time spent 4.63 s, speed 35 steps/s\n",
            "steps[351643]: game [1120], reward [1031.000], eps [0.78], time spent 44.27 s, speed 35 steps/s\n",
            "steps[352155]: game [1121], reward [685.000], eps [0.78], time spent 14.57 s, speed 35 steps/s\n",
            "steps[352195]: game [1122], reward [240.000], eps [0.78], time spent 1.14 s, speed 34 steps/s\n",
            "steps[352337]: game [1123], reward [594.000], eps [0.78], time spent 3.95 s, speed 35 steps/s\n",
            "steps[352494]: game [1124], reward [635.000], eps [0.78], time spent 4.46 s, speed 35 steps/s\n",
            "steps[352702]: game [1125], reward [787.000], eps [0.78], time spent 5.85 s, speed 35 steps/s\n",
            "steps[353022]: game [1126], reward [1018.000], eps [0.77], time spent 9.18 s, speed 34 steps/s\n",
            "steps[353583]: game [1127], reward [685.000], eps [0.77], time spent 16.14 s, speed 34 steps/s\n",
            "steps[353693]: game [1128], reward [623.000], eps [0.77], time spent 3.14 s, speed 35 steps/s\n",
            "steps[353932]: game [1129], reward [620.000], eps [0.77], time spent 6.82 s, speed 35 steps/s\n",
            "steps[354232]: game [1130], reward [783.000], eps [0.77], time spent 8.38 s, speed 35 steps/s\n",
            "steps[354519]: game [1131], reward [1012.000], eps [0.77], time spent 8.09 s, speed 35 steps/s\n",
            "steps[354620]: game [1132], reward [593.000], eps [0.77], time spent 2.83 s, speed 35 steps/s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}