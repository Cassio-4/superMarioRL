{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mediumMario.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFk9LGjJvIG",
        "colab_type": "text"
      },
      "source": [
        "Pip Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMgHLe3JloI",
        "colab_type": "code",
        "outputId": "15f894ac-e3bf-412a-89d2-3dfee20c0424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-super-mario-bros\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/8e/a4a752e0dc7b194620192ebe8682776a4676a57662b3f998ddbcd0660e1c/gym_super_mario_bros-7.2.3-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.0MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0 (from gym-super-mario-bros)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/98/f87eacc9ff3ddfe97ecc889165119317cd4782f5839c24b39f88a1a7e7d7/nes_py-8.1.1.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (0.10.11)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.16.4)\n",
            "Requirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.4.2)\n",
            "Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.3.2->nes-py>=8.0.0->gym-super-mario-bros) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.24.3)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.1-cp36-cp36m-linux_x86_64.whl size=449285 sha256=7cec88ed426c7e6e31a1fde6dbf1c53fcb460c1efa8a61380de3d8f475842b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/d7/e4/0949e4c8947993c5555730a3b15f3cdc5a86507b95388dd608\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.2.3 nes-py-8.1.1\n",
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suz1tERzHnRI",
        "colab_type": "text"
      },
      "source": [
        "Imports e APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx0WGbBECM_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from collections import deque\n",
        "from gym import wrappers\n",
        "from gym.envs.registration import register"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr6xrjGIKko_",
        "colab_type": "text"
      },
      "source": [
        "Environment e \"defines\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5qo5hKXKjo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "input_size = env.observation_space.shape[0]*env.observation_space.shape[1]*3\n",
        "output_size = 6\n",
        "dis = 0.9\n",
        "REPLAY_MEMORY = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXtmVEHtLtJr",
        "colab_type": "text"
      },
      "source": [
        "Classe DQN (DDQN???)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLeyUxnqNr5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN:\n",
        "  \n",
        "    def __init__(self, session, input_size, output_size, name=\"main\"):\n",
        "        self.session = session\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.net_name = name\n",
        "        self._build_network()\n",
        "  \n",
        "    def _build_network(self, h_size=10, l_rate=1e-1):\n",
        "        with tf.variable_scope(self.net_name):\n",
        "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
        "            # First layer of weights\n",
        "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
        "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
        "            layer1 = tf.nn.tanh(tf.matmul(self._X, W1))\n",
        "            # Second layer of Weights\n",
        "            W2 = tf.get_variable(\"W2\", shape=[h_size, self.output_size],\n",
        "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
        "            # Q prediction\n",
        "            self._Qpred = tf.matmul(layer1, W2)\n",
        "        # We need to define the parts of the network needed for learning a policy\n",
        "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
        "        # Loss function\n",
        "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
        "        # Learning\n",
        "        self._train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self._loss)\n",
        "\n",
        "    def predict(self, state):\n",
        "        x = np.reshape(state, [1, self.input_size])\n",
        "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
        "\n",
        "    def update(self, x_stack, y_stack):\n",
        "        return self.session.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MncqROtQSqb7",
        "colab_type": "text"
      },
      "source": [
        "Definições"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9djETCeSswP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replay_train(mainDQN, targetDQN, train_batch):\n",
        "    x_stack = np.empty(0).reshape(0, input_size)\n",
        "    y_stack = np.empty(0).reshape(0, output_size)\n",
        "    \n",
        "    # Get stored information from the buffer\n",
        "    for state, action, reward, next_state, done in train_batch:\n",
        "        Q = mainDQN.predic(state)\n",
        "        \n",
        "    # terminal?\n",
        "        if done:\n",
        "            Q[0, action] = reward\n",
        "        else:\n",
        "            # get target from target DQN (Q')\n",
        "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
        "            \n",
        "        y_stack = np.vstack([y_stack, Q])\n",
        "        x_stack = np.vstack( [x_stack, state])\n",
        "    # Train our network using target and predicted Q values on each episode\n",
        "    return mainDQN.update(x_stack, y_stack)\n",
        "def ddqn_replay_train(mainDQN, targetDQN, train_batch):\n",
        "    '''\n",
        "    Double DQN implementation\n",
        "    :param mainDQN: main DQN\n",
        "    :param targetDQN: target DQN\n",
        "    :param train_batch: minibatch for train\n",
        "    :return: loss\n",
        "    '''\n",
        "    x_stack = np.empty(0).reshape(0, mainDQN.input_size)\n",
        "    y_stack = np.empty(0).reshape(0, mainDQN.output_size)\n",
        "    # Get stored information from the buffer\n",
        "    for state, action, reward, next_state, done in train_batch:\n",
        "        if state is None:                                                               #####why does this happen?\n",
        "            print(\"None State, \", action, \" , \", reward, \" , \", next_state, \" , \", done)\n",
        "        else:\n",
        "            Q = mainDQN.predict(state)\n",
        "            # terminal?\n",
        "            if done:\n",
        "                Q[0, action] = reward\n",
        "            else:\n",
        "                # Double DQN: y = r + gamma * targetDQN(s')[a] where\n",
        "                # a = argmax(mainDQN(s'))\n",
        "                # Q[0, action] = reward + dis * targetDQN.predict(next_state)[0, np.argmax(mainDQN.predict(next_state))]\n",
        "                Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state)) #####use normal one for now\n",
        "                \n",
        "            y_stack = np.vstack([y_stack, Q])\n",
        "            x_stack = np.vstack([x_stack, state.reshape(-1, mainDQN.input_size)])   #####change shape to fit to super mario\n",
        "            \n",
        "    # Train our network using target and predicted Q values on each episode\n",
        "    return mainDQN.update(x_stack, y_stack)\n",
        "\n",
        "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
        "    # Copy variables src_scope to dest_scope\n",
        "    op_holder = []\n",
        "    \n",
        "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
        "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
        "    \n",
        "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
        "        op_holder.append(dest_var.assign(src_var.value()))\n",
        "        \n",
        "    return op_holder\n",
        "\n",
        "def bot_play(mainDQN, env=env):\n",
        "    # See our trained network in action\n",
        "    state = env.reset()\n",
        "    reward_sum = 0\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        reward_sum += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(reward_sum))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnfP0XpzS6dw",
        "colab_type": "text"
      },
      "source": [
        "Main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcUDhevyFdcj",
        "colab_type": "code",
        "outputId": "4cc0f953-2751-4ef6-ff3b-7c5c26ee6ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "\n",
        "            \n",
        "def main():\n",
        "    max_episodes = 5000\n",
        "    # store the previous observations in replay memory\n",
        "    replay_buffer = deque()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
        "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
        "        tf.global_variables_initializer().run()\n",
        "        #initial copy q_net -> target_net\n",
        "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
        "        sess.run(copy_ops)\n",
        "        \n",
        "    for episode in range(max_episodes):\n",
        "        e = 1. / ((episode / 10) + 1)\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        state = env.reset()\n",
        "        \n",
        "        while not done:\n",
        "            if np.random.rand(1) < e or state is None or state.size == 1:           #####why does this happen?\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Choose an action by greedily from the Q-network\n",
        "                #action = np.argmax(mainDQN.predict(state))\n",
        "                action = mainDQN.predict(state).flatten().tolist()                  #####flatten it and change it as a list\n",
        "                for i in range(len(action)):                                        #####the action list has to have only integer 1 or 0\n",
        "                    if action[i] > 0.5 :\n",
        "                        action[i] = 1                                               #####integer 1 only, no 1.0\n",
        "                    else:\n",
        "                        action[i] = 0                                               #####integer 0 only, no 0.0\n",
        "                        \n",
        "            # Get new state and reward from environment\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            if done: # Penalty\n",
        "                reward = -100\n",
        "                \n",
        "            # Save the experience to our buffer\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            if len(replay_buffer) > REPLAY_MEMORY:\n",
        "                replay_buffer.popleft()\n",
        "            \n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "            if step_count > 10000:   # Good enough. Let's move on\n",
        "                break\n",
        "                \n",
        "        print(\"Episode: {} steps: {}\".format(episode, step_count))\n",
        "        if step_count > 10000:\n",
        "            pass\n",
        "            # break\n",
        "    \n",
        "        if episode % 10 == 1: # train every 10 episode\n",
        "            # Get a random batch of experiences\n",
        "            for _ in range(50):\n",
        "                minibatch = random.sample(replay_buffer, 10)\n",
        "                loss, _ = ddqn_replay_train(mainDQN, targetDQN, minibatch)\n",
        "\n",
        "            print(\"Loss: \", loss)\n",
        "            # copy q_net -> target_net\n",
        "            sess.run(copy_ops)\n",
        "\n",
        "# See our trained bot in action\n",
        "        env2 = wrappers.Monitor(env, 'gym-results', force=True)\n",
        "for i in range(200):\n",
        "            bot_play(mainDQN, env=env2)\n",
        "env2.close()\n",
        "        # gym.upload(\"gym-results\", api_key=\"sk_VT2wPcSSOylnlPORltmQ\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bb9bf0c4a9f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gym-results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mbot_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# gym.upload(\"gym-results\", api_key=\"sk_VT2wPcSSOylnlPORltmQ\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mainDQN' is not defined"
          ]
        }
      ]
    }
  ]
}