{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kerasMario.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZX37H8liukb",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/vivek3141/super-mario-rl/blob/master/Keras/keras-rl_deepq.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cleor9Y-TvxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CIcBFyETwgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import gym_super_mario_bros\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import cv2\n",
        "from IPython.display import Image\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbAQzpqouany",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPISODES = 5000\n",
        "\n",
        "#Classe que define o agente\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        #dimensão de um estado (Imagem original de 240x256 pxls com tres camadas)\n",
        "        #Haverá de corte e redimensão\n",
        "        self.state_size = state_size\n",
        "        #Tamanho do conjunto de ações possíveis, depende do wrapper utilizado\n",
        "        self.action_size = action_size\n",
        "        #Memoria de replay, funciona como uma fila onde os ultimos 'maxlen'\n",
        "        #estados são armazenados, também conhecido como 'Experience Replay Buffer'\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        # Taxa de desconto, define a visibilidade do agente\n",
        "        # 1.0 -> um agente com visibilidade perfeita\n",
        "        # 0.0 -> agente so considera recompensa imediata\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        # Taxa de aleatoriedade, sendo 1.0 aleatoriedade total. Utilizado pelo\n",
        "        # metodo Epsilon-Greedy como uma alternativa ao dilema de exploration x\n",
        "        # Exploitation. Começamos com 1.0 para popularmos a memoria de replay \n",
        "        # e começamos a decair esse valor até 0.02 (2% de aleatoriedade)\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        # Taxa de aprendizado, define o quao rapido o modelo se adapta ao problema.\n",
        "        # Representa a diferença a ser atualizada nos pesos da rede através do \n",
        "        # Backpropagation\n",
        "        self.learning_rate = 0.001\n",
        "        # Constroi a rede \n",
        "        self.model = self._build_model()\n",
        "        # Constroi a rede alvo\n",
        "        # Uma rede alvo retorna o valor de Q(a', s') e é atualizada a cada\n",
        "        # nK iterações, sendo uma copia da rede original. \n",
        "        self.target_model = self._build_model()\n",
        "        # Copia os pesos da rede de treino para a rede alvo\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _huber_loss(self, target, prediction):\n",
        "        # sqrt(1+error^2)-1\n",
        "        error = prediction - target\n",
        "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=self._huber_loss,\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # copy weights from model to target_model\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            #print(state.shape)\n",
        "            target = self.model.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                # a = self.model.predict(next_state)[0]\n",
        "                t = self.target_model.predict(next_state)[0]\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw77bSsaTZ6Q",
        "colab_type": "code",
        "outputId": "5524aff6-4691-4a78-dc19-4a9190d82fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"SuperMarioBros-v0\")\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    state_size = env.observation_space.shape[1]\n",
        "\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    # agent.load(\"./save/cartpole-ddqn.h5\")\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
        "        #state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            # env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "          \n",
        "            next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
        "            if(time == 3):\n",
        "                print(next_state.shape)\n",
        "                cv2_imshow(next_state)\n",
        "            reward = reward if not done else -10\n",
        "            #next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAADwCAAAAADK/OUXAAAGz0lEQVR4nO2dq27jWhSGv1aW8hhl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\nAcmhTIHZxer1uDr+D/50idndh5ZzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x240 at 0x7F307404DCC0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f6323ecacc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-6cbcc0c3a461>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# target[0][action] = reward + self.gamma * t[np.argmax(a)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}