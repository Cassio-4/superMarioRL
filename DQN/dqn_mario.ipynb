{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_mario.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7asy7Eovq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install import-ipynb\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdcBjZuby5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import import_ipynb\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "Drive = GoogleDrive(gauth)\n",
        "#importando wrapper\n",
        "# https://drive.google.com/open?id=1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA\n",
        "my_wrapper = Drive.CreateFile({'id':'1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA'})\n",
        "my_wrapper.GetContentFile('env_wrapper.ipynb')\n",
        "import env_wrapper as wrappers\n",
        "#importando modelo de rede dqn\n",
        "#https://drive.google.com/open?id=14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd\n",
        "dqn_model = Drive.CreateFile({'id':'14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd'})\n",
        "dqn_model.GetContentFile('dqn.ipynb')\n",
        "import dqn as dqn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q86nQjqFPl08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyFXhPMqx2PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls \"/content/drive/My Drive/DQN\"\n",
        "%cd \"/content/drive/My Drive/DQN/Logs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QJ6muFIqGf9",
        "colab_type": "code",
        "outputId": "c71a0d9c-c06e-4aee-946e-d47d289e94d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DQN/Logs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dqr0pbfZDnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFOKo3ckCMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-RBqvaZGy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
        "\n",
        "# Valor de Gamma usado na aproximação de Bellman\n",
        "GAMMA = 0.99\n",
        "# O tamanho da 'batch' amostrada do Replay Buffer\n",
        "BATCH_SIZE = 32\n",
        "# Capacidade máxima do Replay Buffer\n",
        "REPLAY_SIZE = 10000\n",
        "# Número de frames que esperamos antes de começar a treinar para popular o Replay Buffer\n",
        "REPLAY_START_SIZE = 10000\n",
        "# Taxa de aprendizado usada pelo otimizador Adam, que é usado nesse código\n",
        "LEARNING_RATE = 1e-4\n",
        "# Frequência de sincronização dos pesos do modelo de treino para o modelo alvo.\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "\"\"\" Parâmetros de declínio de Epsilon. No começo do treinamento começamos com\n",
        "Epsilon=1.0, ou seja, com ações totalmente aleatórias. Depois, durante os 100K\n",
        "primeiros frames, Epsilon decai linearmente para 0.02,\"\"\"\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "\"\"\" Pasta onde salvaremos os logs  \"\"\"\n",
        "train_log_dir = \"/content/drive/My Drive/DQN/Logs/\"\n",
        "\"\"\"Parametros de retomar treino\"\"\"\n",
        "LOAD_NET = True\n",
        "LOAD_REPLAY_BUFFER = True\n",
        "step_index = 2209310\n",
        "games_played = 8800\n",
        "epsilon = 0.02#EPSILON_START\n",
        "\"\"\"Bufferizando para depois inserir no log\"\"\"\n",
        "log_array = []\n",
        "step_epsilon_loss_array = []\n",
        "games_array = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptbVqOXou6yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Código referente ao Replay Buffer. A cada Step no ambiente colocamos a \n",
        "transição no Buffer, mantendo apenas um número fixo de Steps, nesse caso 10k \n",
        "transições. Para treinamento, selecionamos aleatóriamente um lote (batch) de\n",
        "transições do Replay Buffer, o que permite que quebremos a correlação entre \n",
        "passos subsequentes no ambiente.\n",
        "\"\"\"\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.bool), np.array(next_states)\n",
        "\n",
        "\"\"\" \n",
        "Agente:interage com o ambiente e salva o resultado da interação no \n",
        "experience replay buffer\n",
        "\"\"\"\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Na inicialização do Agente guardamos referências para o ambiente (env) e \n",
        "    para o experience replay buffer, registrando a observação(observation) atual\n",
        "    e a recompensa total acumulada até então.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "    \"\"\"\n",
        "    Faz um step no ambiente e guarda o resultado no Buffer. Com probabilidade \n",
        "    Epsilon tomamos uma ação aleatória, caso contrário utilizamos o modelo para \n",
        "    obter os Q-values para todas as possíveis ações e escolhemos a melhor.\n",
        "    \"\"\"\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            epsilon_action = True\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            epsilon_action = False\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "        \"\"\"\n",
        "        Passamos a ação escolhida para o ambiente e pegamos a próxima observation\n",
        "        e recompensa, guardamos os dados no experience buffer e tratamos o fim-\n",
        "        -de-episodio. O resultado dessa função é a recompensa total acumulada se\n",
        "        chegamos ao fim-de-episodio com esse step ou None caso contrário.\n",
        "        \"\"\"\n",
        "        new_state, reward, is_done, info = self.env.step(action)\n",
        "        #log\n",
        "        step_epsilon_loss_array.append(step_index)\n",
        "        if epsilon_action:\n",
        "            step_epsilon_loss_array.append(1)\n",
        "        else:\n",
        "            step_epsilon_loss_array.append(0)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward, info\n",
        "\n",
        "\"\"\"\n",
        "Função que calcula a Loss para a batch amostrada.\n",
        "Argumentos: a batch como uma tupla de arrays (método sample() do experience \n",
        "buffer), nossa rede de treino e a rede alvo, periodicamente sincronizada com a \n",
        "de treino.\n",
        "\"\"\"\n",
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    \"\"\"\n",
        "    Empacota arrays individuais NumPy com dados do lote(batch) em tensores\n",
        "    PyTorch e copia para a GPU\n",
        "    \"\"\"\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "    \"\"\"\n",
        "    Passa as observations para a primeira rede e extrai os Q-Values específicos\n",
        "    para as ações tomadas (?) usando a operação de tensor gather().\n",
        "    \"\"\"\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    \"\"\"\n",
        "    Aplica a rede alvo para as observações de próximo estado e calcula o máximo \n",
        "    Q-Value ao longo da mesma dimensão 1 de ação(?). A função max() retorna \n",
        "    ambos os valores máximos e seus índices (max e argmax), entretanto neste \n",
        "    caso, estamos apenas interesados nos valores, então pegamos a primeira \n",
        "    entrada do resultado\n",
        "    \"\"\"\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    \"\"\"\n",
        "    Aqui há um ponto simples porém, importante: se a transição no batch é do \n",
        "    último step do episódio, então nosso valor da ação não tem uma recompensa com\n",
        "    desconto do próximo estado, visto que não há próximo estado de onde tirar \n",
        "    uma recompensa. Pode parecer pequeno mas é muito importante na prática, sem \n",
        "    isso o treino NÃO irá convergir\n",
        "    \"\"\"\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "    \"\"\"\n",
        "    Calcula o valor da aproximação de Bellman e o erro quadrático médio (MSE)\n",
        "    (loss)\n",
        "    \"\"\"\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "def write_logs(steps, games):\n",
        "    with open('steps_epsilon_loss.csv', 'a', newline='') as steps_log:\n",
        "        steps_writer = csv.writer(steps_log)\n",
        "        for entry in steps:\n",
        "            steps_writer.writerow([entry[0], entry[1], entry[2]])\n",
        "        steps_log.close()\n",
        "        log_array.clear()\n",
        "    \n",
        "    with open('games_log.csv', 'a', newline='') as games_log:\n",
        "        games_writer = csv.writer(games_log)\n",
        "        for entry in games:\n",
        "            games_writer.writerow([entry[0], entry[1], entry[2], entry[3], entry[4]])\n",
        "        games_log.close()\n",
        "        games_array.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbZn2CfYYEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e49cc7f7-e8a0-49f7-aa74-222b3116b213"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Se cuda estiver disponível setamos device como cuda\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    \"\"\"\n",
        "    Cria ambiente (env) com todos os empacotadores aplicados, a rede neural que \n",
        "    será treinada e a rede alvo com a mesma arquitetura. No início serão \n",
        "    inicializadas com pesos diferentes, mas isso não importa visto que serão \n",
        "    sincronizadas a cada SYNC_TARGET_FRAMES, o que corresponde aproximadamente\n",
        "    a um episódio de Pong (quantos frames tem um episódio de Mario?)\n",
        "    \"\"\"\n",
        "    env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
        "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    #Se vamos retomar o treino, carregamos a rede com os parametros salvos e \n",
        "    #sincronizamos com a target_network \n",
        "    if LOAD_NET:\n",
        "        print(\"Reloading net: game %d, step_index %d, epsilon %.2f\" % \n",
        "            (games_played, step_index, epsilon))\n",
        "        net.load_state_dict(torch.load('best.dat', map_location=lambda storage, loc: storage))\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "    \"\"\"\n",
        "    Cria o experience replay buffer de tamanho REPLAY_SIZE e o passa para o agente.\n",
        "    Epsilon é inicializado com 1.0, mas decaira a cada iteração.\n",
        "    \"\"\"\n",
        "    #Se retomar treino, carrega o replay buffer\n",
        "    if LOAD_REPLAY_BUFFER:\n",
        "        print(\"Reloading Replay Buffer\")\n",
        "        with open('experienceBuffer', 'rb') as filehandler: \n",
        "            buffer = pickle.load(filehandler)\n",
        "    else:\n",
        "        buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    \"\"\"\n",
        "    A última parte antes do loop de treino consiste em: criar o otimizador e \n",
        "    variavéis de tempo.Toda vez que a executarmos um numero arbitrario de jogos,\n",
        "    salvamos o modelo em um arquivo.\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    ts = time.time()\n",
        "\n",
        "    while True:\n",
        "        \"\"\"\n",
        "        No início do loop de treino, contamos o número de iterações completadas e \n",
        "        decrescemos Epsilon de acordo com o planejado. Epsilon decairá linearmente \n",
        "        durante o número de frames dados (EPSILON_DECAY_LAST_FRAME) e será mantido\n",
        "        no mesmo nível que EPSILON_FINAL.\n",
        "        \"\"\"\n",
        "        step_index += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - step_index / EPSILON_DECAY_LAST_FRAME)\n",
        "        \"\"\"\n",
        "        Faz o agente dar um único step (usando a rede atual e o valor de epsilon).\n",
        "        Essa função não retorna None apenas se este Step for o último do episódio,\n",
        "        nesse caso, relatamos nosso progresso. Especificamente, calcula-se e \n",
        "        mostra tanto no console quanto no log, esses valores:\n",
        "        - Contagem de episódios reproduzidos\n",
        "        - Valor atual de Epsilon\n",
        "        \"\"\"\n",
        "        reward, info = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            games_played += 1\n",
        "            time_spent_game = time.time() - ts\n",
        "            ts = time.time()\n",
        "            print(\"steps[%d]: game [%d], reward [%.3f], eps [%.2f], time spent %.2f s\" % (\n",
        "                step_index, games_played, reward, epsilon, time_spent_game\n",
        "            ))\n",
        "            games_array.append([games_played, reward, epsilon, time_spent_game, info.get(\"x_pos\")])\n",
        "            # Salva a rede, replay_buffer e preenche logs a cada 'x' jogos\n",
        "            if games_played % 100 == 0:\n",
        "                torch.save(net.state_dict(), 'best.dat')\n",
        "                with open('experienceBuffer', 'wb') as filehandler:\n",
        "                    pickle.dump(buffer, filehandler)\n",
        "                write_logs(log_array, games_array)\n",
        "                print(\"Checkpoint! Game: %d, reward: %.3f, epsilon %.2f\" % \n",
        "                    (games_played, reward, epsilon))\n",
        "                with open('checkpoint.txt', 'w', newline='') as txt_log:\n",
        "                    txt_log.write(\"Checkpoint! Game #%d, step_index: %d, eps: %.2f\" \n",
        "                                  % (games_played, step_index, epsilon))\n",
        "            if games_played > 10000:\n",
        "                print(\"Done %d games_played!\" % games_played)\n",
        "                break\n",
        "        \"\"\"\n",
        "        Checa se o Replay Buffer é grande o bastante para começar o treino. No\n",
        "        começo devemos esperar REPLAY_SIZE transições. A condição seguinte \n",
        "        sincroniza os parâmetros da rede principal e da rede alvo a cada SYNC_TARGET_FRAMES.\n",
        "        \"\"\"\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            step_epsilon_loss_array.clear()\n",
        "            continue\n",
        "        if step_index % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "        \"\"\"\n",
        "        A última parte do loop de treino é simples, mas requer a maior parte \n",
        "        para executar: zera os gradientes, amostra lotes de dados do experience \n",
        "        replay buffer, calcula perca (loss) e faz o passo de otimização para \n",
        "        minimizar a perca.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "        # log\n",
        "        step_epsilon_loss_array.append(loss_t.item())\n",
        "        log_array.append(step_epsilon_loss_array.copy())\n",
        "        step_epsilon_loss_array.clear()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "steps[2654071]: game [9897], reward [1692.000], eps [0.02], time spent 4.69 s\n",
            "steps[2654545]: game [9898], reward [1970.000], eps [0.02], time spent 12.59 s\n",
            "steps[2654574]: game [9899], reward [248.000], eps [0.02], time spent 0.77 s\n",
            "steps[2654755]: game [9900], reward [1675.000], eps [0.02], time spent 4.80 s\n",
            "Checkpoint! Game: 9900, reward: 1675.000, epsilon 0.02\n",
            "steps[2654921]: game [9901], reward [1343.000], eps [0.02], time spent 10.73 s\n",
            "steps[2655149]: game [9902], reward [1936.000], eps [0.02], time spent 6.30 s\n",
            "steps[2655425]: game [9903], reward [2358.000], eps [0.02], time spent 7.91 s\n",
            "steps[2655557]: game [9904], reward [1327.000], eps [0.02], time spent 3.85 s\n",
            "steps[2656123]: game [9905], reward [2552.000], eps [0.02], time spent 15.53 s\n",
            "steps[2656342]: game [9906], reward [1937.000], eps [0.02], time spent 5.84 s\n",
            "steps[2656586]: game [9907], reward [795.000], eps [0.02], time spent 6.48 s\n",
            "steps[2658591]: game [9908], reward [1915.000], eps [0.02], time spent 52.51 s\n",
            "steps[2658754]: game [9909], reward [706.000], eps [0.02], time spent 4.28 s\n",
            "steps[2658963]: game [9910], reward [1662.000], eps [0.02], time spent 5.51 s\n",
            "steps[2659614]: game [9911], reward [1601.000], eps [0.02], time spent 17.09 s\n",
            "steps[2660060]: game [9912], reward [3032.000], eps [0.02], time spent 11.96 s\n",
            "steps[2660510]: game [9913], reward [2319.000], eps [0.02], time spent 11.96 s\n",
            "steps[2660911]: game [9914], reward [2327.000], eps [0.02], time spent 10.52 s\n",
            "steps[2661202]: game [9915], reward [1014.000], eps [0.02], time spent 7.70 s\n",
            "steps[2662169]: game [9916], reward [2468.000], eps [0.02], time spent 25.29 s\n",
            "steps[2663520]: game [9917], reward [2138.000], eps [0.02], time spent 35.67 s\n",
            "steps[2664000]: game [9918], reward [2321.000], eps [0.02], time spent 13.04 s\n",
            "steps[2664654]: game [9919], reward [2282.000], eps [0.02], time spent 17.44 s\n",
            "steps[2664748]: game [9920], reward [630.000], eps [0.02], time spent 2.55 s\n",
            "steps[2666753]: game [9921], reward [2380.000], eps [0.02], time spent 52.97 s\n",
            "steps[2667244]: game [9922], reward [2601.000], eps [0.02], time spent 12.88 s\n",
            "steps[2667403]: game [9923], reward [812.000], eps [0.02], time spent 4.17 s\n",
            "steps[2667830]: game [9924], reward [2327.000], eps [0.02], time spent 11.20 s\n",
            "steps[2668554]: game [9925], reward [2452.000], eps [0.02], time spent 18.86 s\n",
            "steps[2668692]: game [9926], reward [816.000], eps [0.02], time spent 3.62 s\n",
            "steps[2669412]: game [9927], reward [2267.000], eps [0.02], time spent 18.83 s\n",
            "steps[2669575]: game [9928], reward [1574.000], eps [0.02], time spent 4.25 s\n",
            "steps[2671580]: game [9929], reward [1609.000], eps [0.02], time spent 51.95 s\n",
            "steps[2671884]: game [9930], reward [1322.000], eps [0.02], time spent 8.00 s\n",
            "steps[2672473]: game [9931], reward [3004.000], eps [0.02], time spent 15.40 s\n",
            "steps[2672706]: game [9932], reward [1700.000], eps [0.02], time spent 6.09 s\n",
            "steps[2673072]: game [9933], reward [2339.000], eps [0.02], time spent 9.57 s\n",
            "steps[2673179]: game [9934], reward [729.000], eps [0.02], time spent 2.80 s\n",
            "steps[2673313]: game [9935], reward [817.000], eps [0.02], time spent 3.52 s\n",
            "steps[2673752]: game [9936], reward [2597.000], eps [0.02], time spent 11.40 s\n",
            "steps[2673995]: game [9937], reward [732.000], eps [0.02], time spent 6.31 s\n",
            "steps[2676000]: game [9938], reward [2341.000], eps [0.02], time spent 51.93 s\n",
            "steps[2676062]: game [9939], reward [610.000], eps [0.02], time spent 1.61 s\n",
            "steps[2676269]: game [9940], reward [1691.000], eps [0.02], time spent 5.44 s\n",
            "steps[2678274]: game [9941], reward [2385.000], eps [0.02], time spent 52.11 s\n",
            "steps[2678407]: game [9942], reward [817.000], eps [0.02], time spent 3.50 s\n",
            "steps[2678489]: game [9943], reward [614.000], eps [0.02], time spent 2.11 s\n",
            "steps[2678550]: game [9944], reward [612.000], eps [0.02], time spent 1.60 s\n",
            "steps[2678746]: game [9945], reward [1926.000], eps [0.02], time spent 5.18 s\n",
            "steps[2678886]: game [9946], reward [1340.000], eps [0.02], time spent 3.63 s\n",
            "steps[2678953]: game [9947], reward [610.000], eps [0.02], time spent 1.76 s\n",
            "steps[2679063]: game [9948], reward [579.000], eps [0.02], time spent 2.89 s\n",
            "steps[2679303]: game [9949], reward [1686.000], eps [0.02], time spent 6.29 s\n",
            "steps[2679665]: game [9950], reward [2342.000], eps [0.02], time spent 9.44 s\n",
            "steps[2679729]: game [9951], reward [611.000], eps [0.02], time spent 1.69 s\n",
            "steps[2679869]: game [9952], reward [777.000], eps [0.02], time spent 3.68 s\n",
            "steps[2680069]: game [9953], reward [1929.000], eps [0.02], time spent 5.26 s\n",
            "steps[2680266]: game [9954], reward [1926.000], eps [0.02], time spent 5.22 s\n",
            "steps[2680362]: game [9955], reward [614.000], eps [0.02], time spent 2.53 s\n",
            "steps[2680509]: game [9956], reward [638.000], eps [0.02], time spent 3.84 s\n",
            "steps[2680782]: game [9957], reward [1404.000], eps [0.02], time spent 7.12 s\n",
            "steps[2680900]: game [9958], reward [628.000], eps [0.02], time spent 3.09 s\n",
            "steps[2682071]: game [9959], reward [2887.000], eps [0.02], time spent 30.29 s\n",
            "steps[2682769]: game [9960], reward [2982.000], eps [0.02], time spent 18.34 s\n",
            "steps[2682980]: game [9961], reward [1417.000], eps [0.02], time spent 5.52 s\n",
            "steps[2683121]: game [9962], reward [1336.000], eps [0.02], time spent 3.71 s\n",
            "steps[2683444]: game [9963], reward [1674.000], eps [0.02], time spent 8.50 s\n",
            "steps[2684968]: game [9964], reward [2817.000], eps [0.02], time spent 39.91 s\n",
            "steps[2685286]: game [9965], reward [2348.000], eps [0.02], time spent 8.33 s\n",
            "steps[2685472]: game [9966], reward [1693.000], eps [0.02], time spent 4.92 s\n",
            "steps[2685658]: game [9967], reward [1693.000], eps [0.02], time spent 4.94 s\n",
            "steps[2685806]: game [9968], reward [814.000], eps [0.02], time spent 3.90 s\n",
            "steps[2686016]: game [9969], reward [1921.000], eps [0.02], time spent 5.51 s\n",
            "steps[2686157]: game [9970], reward [1064.000], eps [0.02], time spent 3.69 s\n",
            "steps[2686354]: game [9971], reward [1654.000], eps [0.02], time spent 5.21 s\n",
            "steps[2688359]: game [9972], reward [2544.000], eps [0.02], time spent 52.24 s\n",
            "steps[2688712]: game [9973], reward [2347.000], eps [0.02], time spent 9.36 s\n",
            "steps[2688865]: game [9974], reward [813.000], eps [0.02], time spent 4.05 s\n",
            "steps[2690632]: game [9975], reward [419.000], eps [0.02], time spent 46.18 s\n",
            "steps[2690791]: game [9976], reward [1345.000], eps [0.02], time spent 4.16 s\n",
            "steps[2690902]: game [9977], reward [562.000], eps [0.02], time spent 2.93 s\n",
            "steps[2691361]: game [9978], reward [1524.000], eps [0.02], time spent 12.22 s\n",
            "steps[2691464]: game [9979], reward [563.000], eps [0.02], time spent 2.74 s\n",
            "steps[2691603]: game [9980], reward [743.000], eps [0.02], time spent 3.68 s\n",
            "steps[2693608]: game [9981], reward [267.000], eps [0.02], time spent 53.62 s\n",
            "steps[2695282]: game [9982], reward [2787.000], eps [0.02], time spent 44.61 s\n",
            "steps[2695587]: game [9983], reward [2351.000], eps [0.02], time spent 8.23 s\n",
            "steps[2695927]: game [9984], reward [1595.000], eps [0.02], time spent 9.22 s\n",
            "steps[2696587]: game [9985], reward [2989.000], eps [0.02], time spent 17.63 s\n",
            "steps[2696689]: game [9986], reward [563.000], eps [0.02], time spent 2.73 s\n",
            "steps[2696863]: game [9987], reward [1549.000], eps [0.02], time spent 4.69 s\n",
            "steps[2696967]: game [9988], reward [563.000], eps [0.02], time spent 2.76 s\n",
            "steps[2697601]: game [9989], reward [2285.000], eps [0.02], time spent 16.92 s\n",
            "steps[2697743]: game [9990], reward [1329.000], eps [0.02], time spent 3.83 s\n",
            "steps[2697889]: game [9991], reward [1041.000], eps [0.02], time spent 3.93 s\n",
            "steps[2698091]: game [9992], reward [1616.000], eps [0.02], time spent 5.48 s\n",
            "steps[2698471]: game [9993], reward [2337.000], eps [0.02], time spent 10.16 s\n",
            "steps[2698666]: game [9994], reward [1913.000], eps [0.02], time spent 5.26 s\n",
            "steps[2699456]: game [9995], reward [2964.000], eps [0.02], time spent 21.06 s\n",
            "steps[2700801]: game [9996], reward [2526.000], eps [0.02], time spent 35.89 s\n",
            "steps[2701107]: game [9997], reward [2353.000], eps [0.02], time spent 8.19 s\n",
            "steps[2701745]: game [9998], reward [2994.000], eps [0.02], time spent 17.09 s\n",
            "steps[2701981]: game [9999], reward [1627.000], eps [0.02], time spent 6.37 s\n",
            "steps[2702380]: game [10000], reward [2335.000], eps [0.02], time spent 10.67 s\n",
            "Checkpoint! Game: 10000, reward: 2335.000, epsilon 0.02\n",
            "steps[2704385]: game [10001], reward [2366.000], eps [0.02], time spent 62.52 s\n",
            "Done 10001 games_played!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}