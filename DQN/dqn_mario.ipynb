{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_mario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cassio-4/superMarioRL/blob/master/DQN/dqn_mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPaonxN41DMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7asy7Eovq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install import-ipynb\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdcBjZuby5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import import_ipynb\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "#importando wrapper\n",
        "# https://drive.google.com/open?id=1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA\n",
        "my_wrapper = drive.CreateFile({'id':'1hk5V6dPPgcAFNCqTlDwQRM_dlqdxCEwA'})\n",
        "my_wrapper.GetContentFile('env_wrapper.ipynb')\n",
        "import env_wrapper as wrappers\n",
        "#importando modelo de rede dqn\n",
        "#https://drive.google.com/open?id=14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd\n",
        "dqn_model = drive.CreateFile({'id':'14wjjrd_iI1y1x-N01PJaDBzqC43c1ipd'})\n",
        "dqn_model.GetContentFile('dqn.ipynb')\n",
        "import dqn as dqn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dqr0pbfZDnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from tensorflow import summary\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFOKo3ckCMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-RBqvaZGy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_ENV_NAME = 'SuperMarioBros-v0'\n",
        "# Reward boundary for the last 100 episodes to stop training. \n",
        "#MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "# Valor de Gamma usado na aproximação de Bellman\n",
        "GAMMA = 0.99\n",
        "# O tamanho da 'batch' amostrada do Replay Buffer\n",
        "BATCH_SIZE = 32\n",
        "# Capacidade máxima do Replay Buffer\n",
        "REPLAY_SIZE = 10000\n",
        "# Número de frames que esperamos antes de começar a treinar para popular o Replay Buffer\n",
        "REPLAY_START_SIZE = 10000\n",
        "# Taxa de aprendizado usada pelo otimizador Adam, que é usado nesse código\n",
        "LEARNING_RATE = 1e-4\n",
        "# Frequência de sincronização dos pesos do modelo de treino para o modelo alvo.\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "\"\"\" Parâmetros de declínio de Epsilon. No começo do treinamento começamos com\n",
        "Epsilon=1.0, ou seja, com ações totalmente aleatórias. Depois, durante os 100K\n",
        "primeiros frames, Epsilon decai linearmente para 0.02,\"\"\"\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptbVqOXou6yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Código referente ao Replay Buffer. A cada Step no ambiente colocamos a \n",
        "transição no Buffer, mantendo apenas um número fixo de Steps, nesse caso 10k \n",
        "transições. Para treinamento, selecionamos aleatóriamente um lote (batch) de\n",
        "transições do Replay Buffer, o que permite que quebremos a correlação entre \n",
        "passos subsequentes no ambiente.\n",
        "\"\"\"\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "\"\"\" \n",
        "Agente:interage com o ambiente e salva o resultado da interação no \n",
        "experience replay buffer\n",
        "\"\"\"\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Na inicialização do Agente guardamos referências para o ambiente (env) e \n",
        "    para o experience replay buffer, registrando a observação(observation) atual\n",
        "    e a recompensa total acumulada até então.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "    \"\"\"\n",
        "    Faz um step no ambiente e guarda o resultado no Buffer. Com probabilidade \n",
        "    Epsilon tomamos uma ação aleatória, caso contrário utilizamos o modelo para \n",
        "    obter os Q-values para todas as possíveis ações e escolhemos a melhor.\n",
        "    \"\"\"\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "        \"\"\"\n",
        "        Passamos a ação escolhida para o ambiente e pegamos a próxima observation\n",
        "        e recompensa, guardamos os dados no experience buffer e tratamos o fim-\n",
        "        -de-episodio. O resultado dessa função é a recompensa total acumulada se\n",
        "        chegamos ao fim-de-episodio com esse step ou None caso contrário.\n",
        "        \"\"\"\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n",
        "\n",
        "\"\"\"\n",
        "Função que calcula a Loss para a batch amostrada.\n",
        "Argumentos: a batch como uma tupla de arrays (método sample() do experience \n",
        "buffer), nossa rede de treino e a rede alvo, periodicamente sincronizada com a \n",
        "de treino.\n",
        "\"\"\"\n",
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    \"\"\"\n",
        "    Empacota arrays individuais NumPy com dados do lote(batch) em tensores\n",
        "    PyTorch e copia para a GPU\n",
        "    \"\"\"\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.ByteTensor(dones).to(device)\n",
        "    \"\"\"\n",
        "    Passa as observations para a primeira rede e extrai os Q-Values específicos\n",
        "    para as ações tomadas (?) usando a operação de tensor gather().\n",
        "    \"\"\"\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    \"\"\"\n",
        "    Aplica a rede alvo para as observações de próximo estado e calcula o máximo \n",
        "    Q-Value ao longo da mesma dimensão 1 de ação(?). A função max() retorna \n",
        "    ambos os valores máximos e seus índices (max e argmax), entretanto neste \n",
        "    caso, estamos apenas interesados nos valores, então pegamos a primeira \n",
        "    entrada do resultado\n",
        "    \"\"\"\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    \"\"\"\n",
        "    Aqui há um ponto simples porém, importante: se a transição no batch é do \n",
        "    último step do episódio, então nosso valor da ação não tem uma recompensa com\n",
        "    desconto do próximo estado, visto que não há próximo estado de onde tirar \n",
        "    uma recompensa. Pode parecer pequeno mas é muito importante na prática, sem \n",
        "    isso o treino NÃO irá convergir\n",
        "    \"\"\"\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "    \"\"\"\n",
        "    Calcula o valor da aproximação de Bellman e o erro quadrático médio (MSE)\n",
        "    (loss)\n",
        "    \"\"\"\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dXh7JdQESYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import traceback\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
        "\n",
        "    log = file if hasattr(file,'write') else sys.stderr\n",
        "    traceback.print_stack(file=log)\n",
        "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
        "\n",
        "warnings.showwarning = warn_with_traceback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbZn2CfYYEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Se cuda estiver disponível setamos device como cuda\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    \"\"\"\n",
        "    Cria ambiente (env) com todos os empacotadores aplicados, a rede neural que \n",
        "    será treinada e a rede alvo com a mesma arquitetura. No início serão \n",
        "    inicializadas com pesos diferentes, mas isso não importa visto que serão \n",
        "    sincronizadas a cada SYNC_TARGET_FRAMES, o que corresponde aproximadamente\n",
        "    a um episódio de Pong (quantos frames tem um episódio de Mario?)\n",
        "    \"\"\"\n",
        "    env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
        "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    \n",
        "    current_time = str(datetime.datetime.now())\n",
        "    train_log_dir = 'logs/' + current_time\n",
        "    writer = summary.create_file_writer(train_log_dir)\n",
        "    #writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
        "    \"\"\"\n",
        "    Cria o experience replay buffer de tamanho REPLAY_SIZE e o passa para o agente.\n",
        "    Epsilon é inicializado com 1.0, mas decaira a cada iteração.\n",
        "    \"\"\"\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    epsilon = EPSILON_START\n",
        "    \n",
        "    \"\"\"\n",
        "    A última parte antes do loop de treino consiste em: criar o otimizador, um \n",
        "    buffer para recompensas de episodios completos (full episode rewards(?)), um\n",
        "    contador de frames e variaveis para rastrear velocidade, e a melhor \n",
        "    recompensa média alcançada.Toda vez que a recompensa média alcançada for \n",
        "    maior que o último recorde, salvamos o modelo em um arquivo.\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_mean_reward = None\n",
        "\n",
        "    while True:\n",
        "        \"\"\"\n",
        "        No início do loop de treino, contamos o número de iterações completadas e \n",
        "        decrescemos Epsilon de acordo com o planejado. Epsilon decairá linearmente \n",
        "        durante o número de frames dados (EPSILON_DECAY_LAST_FRAME) e será mantido\n",
        "        no mesmo nível que EPSILON_FINAL.\n",
        "        \"\"\"\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "        \"\"\"\n",
        "        Faz o agente dar um único step (usando a rede atual e o valor de epsilon).\n",
        "        Essa função não retorna None apenas se este Step for o último do episódio,\n",
        "        nesse caso, relatamos nosso progresso. Especificamente, calcula-se e \n",
        "        mostra tanto no console quanto no TensorBoard, esses valores:\n",
        "        - Velocidade (speed) como uma contagem de frames processados /s\n",
        "        - Contagem de episódios reproduzidos\n",
        "        - Média de recompensa para os últimos 100 episódios\n",
        "        - Valor atual de Epsilon\n",
        "        \"\"\"\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar(\"epsilon\", epsilon, frame_idx)\n",
        "                tf.summary.scalar(\"speed\", speed, frame_idx)\n",
        "                tf.summary.scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "                tf.summary.scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "                #writer.(\"epsilon\", epsilon, frame_idx)\n",
        "                #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "                #writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "                #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            \"\"\"\n",
        "            Toda vez que a recompensa média para os últimos 100 episódios atingir\n",
        "            um máximo, relatamos e salvamos os parâmetros do modelo. \n",
        "            (Para PONG)Se a esta ultrapassar o limite paramos o treino, para Pong\n",
        "            o limite é 19.5 que significa ganhar mais de 19 jogos dos 21 possíveis.\n",
        "            \"\"\"\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > 3000:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "        \"\"\"\n",
        "        Checa se o Replay Buffer é grande o bastante para começar o treino. No\n",
        "        começo devemos esperar REPLAY_SIZE transições. A condição seguinte \n",
        "        sincroniza os parâmetros da rede principal e da rede alvo a cada SYNC_TARGET_FRAMES.\n",
        "        \"\"\"\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "        \"\"\"\n",
        "        A última parte do loop de treino é simples, mas requer a maior parte \n",
        "        para executar: zera os gradientes, amostra lotes de dados do experience \n",
        "        replay buffer, calcula perca (loss) e faz o passo de otimização para \n",
        "        minimizar a perca.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}